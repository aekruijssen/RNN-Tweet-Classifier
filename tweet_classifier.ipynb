{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "TWEETS_DIR = './obama-trump-data/tweets'\n",
    "LABELS_DIR = './obama-trump-data/labels_np'\n",
    "\n",
    "#word embeddings file directory \n",
    "EMBEDDINGS_DIR = 'glove.6B.50d.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tweet:  \"America should be very proud.\" President Obama #LoveWins . Label:  0\n",
      "Tweet:  TO ALL AMERICANS\n",
      "https://t.co/D7Es6ie4fY . Label:  1\n",
      "Tweet:  No deal was made last night on DACA. Massive border security would have to be agreed to in exchange for consent. Would be subject to vote. . Label:  1\n",
      "Tweet:  I was viciously attacked by Mr. Khan at the Democratic Convention. Am I not allowed to respond? Hillary voted for the Iraq war, not me! . Label:  1\n",
      "Tweet:  .@RudyGiuliani, one of the finest people I know and a former GREAT Mayor of N.Y.C., just took himself out of consideration for \"State\". . Label:  1\n",
      "Tweet:  The #TPP establishes the highest labor standards of any trade agreement in history. http://t.co/3vqoancyYp . Label:  0\n",
      "Tweet:  Thoughts and prayers to the great people of Indiana. You will prevail! . Label:  1\n",
      "Tweet:  Hillary said she was under sniper fire (while surrounded by USSS.) Turned out to be a total lie. She is not fit to https://t.co/hBIrGj21l6 . Label:  1\n",
      "Tweet:  The failing @nytimes has become a newspaper of fiction. Their stories about me always quote non-existent unnamed sources. Very dishonest! . Label:  1\n",
      "Tweet:  Thank you Council Bluffs, Iowa! Will be back soon. Remember- everything you need to know about Hillary -- just https://t.co/45kIHxdX83 . Label:  1\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "# Load tweets, labels\n",
    "tweets = pickle.load(open(TWEETS_DIR,'rb'))\n",
    "labels = pickle.load(open(LABELS_DIR,'rb'))\n",
    "\n",
    "# Sample tweets\n",
    "for i in range(0,100,10):\n",
    "    print(\"Tweet: \", tweets[i], \". Label: \", labels[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 13049 unique tokens (words).\n",
      "Training data size is (6136,31)\n",
      "Labels are size 6136\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from keras.preprocessing.text import text_to_word_sequence\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "# Tokenize the tweets (convert sentence to sequence of words)\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(tweets)\n",
    "\n",
    "sequences = tokenizer.texts_to_sequences(tweets)\n",
    "word_index = tokenizer.word_index\n",
    "\n",
    "print('Found %s unique tokens (words).' % len(word_index))\n",
    "\n",
    "# Pad sequences to ensure samples are the same size\n",
    "training_data = pad_sequences(sequences)\n",
    "\n",
    "print(\"Training data size is (%d,%d)\"  % training_data.shape) # shape = (num. tweets, max. length of tweet)\n",
    "print(\"Labels are size %d\"  % labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original tweet:  \"America should be very proud.\" President Obama #LoveWins\n",
      "Tweet after tokenization and padding:  [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0  44  88  21  83 391  12  14 794]\n"
     ]
    }
   ],
   "source": [
    "# Show effect of tokenization, padding\n",
    "print(\"Original tweet: \", tweets[0])\n",
    "print(\"Tweet after tokenization and padding: \", training_data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Indexing word vectors.\n",
      "Found 400000 word vectors.\n"
     ]
    }
   ],
   "source": [
    "# Convert words to word embedding vectors\n",
    "\n",
    "EMBEDDING_DIM = 50\n",
    "print('Indexing word vectors.')\n",
    "\n",
    "import os\n",
    "embeddings_index = {}\n",
    "f = open(EMBEDDINGS_DIR, 'rb') # NOTE: if using Windows and getting errors: change this to open(EMBEDDINGS_DIR, 'rb')\n",
    "for line in f:\n",
    "    values = line.split()\n",
    "    word = values[0].decode('UTF-8') # NOTE: if you made that 'rb' change: add to this line, values[0].decode('UTF-8')\n",
    "    coefs = np.asarray(values[1:], dtype='float32')\n",
    "    embeddings_index[word] = coefs\n",
    "f.close()\n",
    "\n",
    "print('Found %s word vectors.' % len(embeddings_index))\n",
    "\n",
    "# prepare word embedding matrix\n",
    "num_words = len(word_index)+1\n",
    "embedding_matrix = np.zeros((num_words, EMBEDDING_DIM))\n",
    "for word, i in word_index.items():\n",
    "    if i >= num_words:\n",
    "        continue\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        # words not found in embedding index will be all-zeros.\n",
    "        embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2645\n",
      "[ 0.079084   -0.81503999  1.79009998  0.91653001  0.10797    -0.55628002\n",
      " -0.84426999 -1.49510002  0.13417999  0.63626999  0.35146001  0.25813001\n",
      " -0.55028999  0.51055998  0.37408999  0.12092    -1.61660004  0.83653003\n",
      "  0.14202    -0.52348     0.73452997  0.12207    -0.49079001  0.32532999\n",
      "  0.45306    -1.58500004 -0.63848001 -1.00530005  0.10454    -0.42984\n",
      "  3.18099999 -0.62186998  0.16819    -1.01390004  0.064058    0.57844001\n",
      " -0.45559999  0.73782998  0.37202999 -0.57722002  0.66441     0.055129\n",
      "  0.037891    1.32749999  0.30991     0.50696999  1.23570001  0.1274\n",
      " -0.11434     0.20709001]\n"
     ]
    }
   ],
   "source": [
    "# Sample word embedding vector:\n",
    "print(word_index[\"computer\"]) # retrieve word index\n",
    "print(embedding_matrix[2645]) # use word index to retrieve word embedding vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Embedding, Input\n",
    "from keras.layers.merge import Concatenate\n",
    "from keras.layers.core import Dense, Activation, Flatten\n",
    "from keras.layers import Dropout, concatenate\n",
    "from keras.layers.recurrent import LSTM\n",
    "from keras.layers.wrappers import Bidirectional\n",
    "from keras.layers.convolutional import Conv1D, MaxPooling1D\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping, TensorBoard\n",
    "from keras import metrics\n",
    "from keras.models import Model\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\adina\\Anaconda3\\envs\\caispp\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:1238: calling reduce_sum (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "keep_dims is deprecated, use keepdims instead\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 31, 50)            652500    \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 31, 128)           91648     \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 31, 128)           0         \n",
      "_________________________________________________________________\n",
      "lstm_2 (LSTM)                (None, 128)               131584    \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 32)                4128      \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 1)                 33        \n",
      "=================================================================\n",
      "Total params: 879,893\n",
      "Trainable params: 227,393\n",
      "Non-trainable params: 652,500\n",
      "_________________________________________________________________\n",
      "None\n",
      "WARNING:tensorflow:From C:\\Users\\adina\\Anaconda3\\envs\\caispp\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:1340: calling reduce_mean (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "keep_dims is deprecated, use keepdims instead\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "\n",
    "# Add pre-trained embedding layer \n",
    "    # converts word indices to GloVe word embedding vectors as they're fed in\n",
    "model.add(Embedding(len(word_index) + 1,\n",
    "                    EMBEDDING_DIM,\n",
    "                    weights=[embedding_matrix],\n",
    "                    input_length=training_data.shape[1],\n",
    "                    trainable=False))\n",
    "\n",
    "# At this point, each individual training sample is now a sequence of word embedding vectors\n",
    "\n",
    "# First LSTM layer (return sequence so that we can feed the output into the 2nd LSTM layer)\n",
    "model.add(LSTM(128, return_sequences = True, activation='relu'))\n",
    "model.add(Dropout(.2))\n",
    "\n",
    "# Second LSTM layer \n",
    "    # Don't return sequence this time, because we're feeding into a fully-connected layer\n",
    "model.add(LSTM(128, activation='relu'))\n",
    "model.add(Dropout(.2))\n",
    "\n",
    "# Dense 1\n",
    "model.add(Dense(32, activation='relu'))\n",
    "model.add(Dropout(.2))\n",
    "\n",
    "# Dense 2 (final vote)\n",
    "model.add(Dense(1, activation = 'sigmoid'))\n",
    "\n",
    "print(model.summary())\n",
    "\n",
    "LOSS = 'binary_crossentropy' # because we're classifying between 0 and 1\n",
    "OPTIMIZER = 'RMSprop' # RMSprop tends to work well for recurrent models\n",
    "\n",
    "model.compile(loss = LOSS, optimizer = OPTIMIZER, metrics = [metrics.binary_accuracy])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 4908 samples, validate on 1228 samples\n",
      "Epoch 1/8\n",
      "4908/4908 [==============================] - 6s 1ms/step - loss: 0.5701 - binary_accuracy: 0.6899 - val_loss: 0.3661 - val_binary_accuracy: 0.8396\n",
      "Epoch 2/8\n",
      "4908/4908 [==============================] - 4s 900us/step - loss: 0.3764 - binary_accuracy: 0.8443 - val_loss: 0.4529 - val_binary_accuracy: 0.7801\n",
      "Epoch 3/8\n",
      "4908/4908 [==============================] - 5s 976us/step - loss: 0.3077 - binary_accuracy: 0.8753 - val_loss: 0.2398 - val_binary_accuracy: 0.9031\n",
      "Epoch 4/8\n",
      "4908/4908 [==============================] - 4s 906us/step - loss: 0.2415 - binary_accuracy: 0.9040 - val_loss: 0.2142 - val_binary_accuracy: 0.9121\n",
      "Epoch 5/8\n",
      "4908/4908 [==============================] - 4s 895us/step - loss: 0.2344 - binary_accuracy: 0.9097 - val_loss: 0.1892 - val_binary_accuracy: 0.9186\n",
      "Epoch 6/8\n",
      "4908/4908 [==============================] - 4s 889us/step - loss: 0.1896 - binary_accuracy: 0.9209 - val_loss: 0.3058 - val_binary_accuracy: 0.8762\n",
      "Epoch 7/8\n",
      "4908/4908 [==============================] - 5s 921us/step - loss: 0.1862 - binary_accuracy: 0.9254 - val_loss: 0.1588 - val_binary_accuracy: 0.9406\n",
      "Epoch 8/8\n",
      "4908/4908 [==============================] - 5s 917us/step - loss: 0.1569 - binary_accuracy: 0.9356 - val_loss: 0.2048 - val_binary_accuracy: 0.9226\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x28ecfa60278>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "EPOCHS = 8\n",
    "BATCH_SIZE = 150\n",
    "\n",
    "model.fit(training_data, labels, \n",
    "          epochs = EPOCHS, \n",
    "          batch_size = BATCH_SIZE, \n",
    "          validation_split =.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6136/6136 [==============================] - 2s 295us/step\n",
      "Test score: 0.16587420085486332\n",
      "Test accuracy: 0.9341590602480293\n"
     ]
    }
   ],
   "source": [
    "score, acc = model.evaluate(training_data, labels,\n",
    "                            batch_size=BATCH_SIZE)\n",
    "print('Test score:', score)\n",
    "print('Test accuracy:', acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:caispp]",
   "language": "python",
   "name": "conda-env-caispp-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
